{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyedflib\n",
    "from matplotlib import pyplot as plt\n",
    "from nitime import utils\n",
    "from nitime import algorithms as alg\n",
    "from nitime.timeseries import TimeSeries\n",
    "from nitime.viz import plot_tseries\n",
    "import csv\n",
    "import pywt\n",
    "import scipy.stats as sp\n",
    "from sklearn.lda import LDA\n",
    "from scipy import signal\n",
    "from spectrum import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from wyrm import processing as proc\n",
    "from wyrm.types import Data\n",
    "from wyrm.io import convert_mushu_data\n",
    "from sklearn import metrics\n",
    "from wyrm.processing import calculate_csp,segment_dat,apply_csp,append_epo\n",
    "from wyrm.processing import select_channels\n",
    "from wyrm.processing import swapaxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files  = [f for f in listdir('Training data')]\n",
    "\n",
    "\n",
    "channels = ['Fp1', 'AFp1', 'Fpz', 'AFp2', 'Fp2', 'AF7', 'AF3', 'AF4', 'AF8', 'FAF5', 'FAF1', 'FAF2', 'FAF6', \n",
    "                'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FFC7', 'FFC5', 'FFC3', 'FFC1', 'FFC2', 'FFC4', \n",
    "                'FFC6', 'FFC8', 'FT9', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10', 'CFC7', \n",
    "                'CFC5', 'CFC3', 'CFC1', 'CFC2', 'CFC4', 'CFC6', 'CFC8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6'\n",
    "                , 'T8', 'CCP7', 'CCP5', 'CCP3', 'CCP1', 'CCP2', 'CCP4', 'CCP6', 'CCP8', 'TP9', 'TP7', 'CP5', 'CP3', 'CP1',\n",
    "                'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10', 'PCP7', 'PCP5', 'PCP3', 'PCP1', 'PCP2', 'PCP4', 'PCP6', 'PCP8', \n",
    "                'P9', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'P10', 'PPO7', 'PPO5', 'PPO1', 'PPO2', 'PPO6',\n",
    "                'PPO8', 'PO7', 'PO3', 'PO1', 'POz', 'PO2', 'PO4', 'PO8', 'OPO1', 'OPO2', 'O1', 'Oz', 'O2', 'OI1', 'OI2', \n",
    "                'I1', 'I2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 1st subject\n",
    "\n",
    "training_data = 'Training data/' + files[0] + '/data_set_IVa_' + files[0] + '_cnt.txt'\n",
    "markers       = 'Training data/' + files[0] + '/data_set_IVa_' + files[0] + '_mrk.txt'\n",
    "    \n",
    "signal_array = np.loadtxt(training_data)\n",
    "\n",
    "(b, a) = signal.butter(3, np.array([8, 30])/ 100.00, 'bandpass')\n",
    "signal_array = signal.lfilter(b, a,signal_array, 1)\n",
    "            \n",
    "# #signal_array = np.transpose(signal_array)\n",
    "    \n",
    "# marker_array = [map(int, l.split()) for l in open(markers).readlines() ]\n",
    "        \n",
    "time = np.arange(len(signal_array))\n",
    "\n",
    "markers1 = np.loadtxt(markers, dtype = int)\n",
    "    \n",
    "train_markers1 = [(float(events[0]),str(events[1])) for events in markers1 if events[1]!= 0]\n",
    "\n",
    "\n",
    "for events in markers1:\n",
    "    if events[1] != 0:\n",
    "        train_markers1.append((float(events[0]) + 100.0, str(events[1])))\n",
    "        train_markers1.append((float(events[0]) + 200.0, str(events[1])))\n",
    "    \n",
    "\n",
    "markers1 = np.array(train_markers1)\n",
    "\n",
    "y1 = markers1[:, 1]\n",
    "\n",
    "y1 = [int(value) for value in y1]\n",
    "\n",
    "\n",
    "# y1 = [0] * len(markers1)\n",
    "\n",
    "markers_subject1_class_1 = [(float(events[0]), str(events[1])) for events in markers1 if events[1] == '1']\n",
    "markers_subject1_class_2 = [(float(events[0]), str(events[1])) for events in markers1 if events[1] == '2']\n",
    "\n",
    "cnt1 = convert_mushu_data(signal_array, markers_subject1_class_1, 100, channels)\n",
    "cnt2 = convert_mushu_data(signal_array, markers_subject1_class_2, 100, channels)\n",
    "    \n",
    "md = {'class 1': ['1'], 'class 2': ['2']}\n",
    "    \n",
    "epoch_subject1_class1 = segment_dat(cnt1, md, [0, 1000])\n",
    "epoch_subject1_class2 = segment_dat(cnt2, md, [0, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 2nd subject\n",
    "\n",
    "training_data = 'Training data/' + files[1] + '/data_set_IVa_' + files[1] + '_cnt.txt'\n",
    "markers       = 'Training data/' + files[1] + '/data_set_IVa_' + files[1] + '_mrk.txt'\n",
    "    \n",
    "signal_array = np.loadtxt(training_data)\n",
    "\n",
    "(b, a) = signal.butter(3, np.array([8, 30])/ 100.00, 'bandpass')\n",
    "signal_array = signal.lfilter(b, a,signal_array, 1)\n",
    "            \n",
    "#signal_array = np.transpose(signal_array)\n",
    "    \n",
    "# marker_array = [map(int,l.split()) for l in open(markers).readlines()]\n",
    "        \n",
    "time = np.arange(len(signal_array))\n",
    "\n",
    "markers2 = np.loadtxt(markers, dtype = int)\n",
    "train_markers2 = [(float(events[0]),str(events[1])) for events in markers2 if events[1]!= 0]\n",
    "\n",
    "\n",
    "for events in markers2:\n",
    "    if events[1] != 0:\n",
    "        train_markers2.append((float(events[0]) + 100.0,str(events[1])))\n",
    "        train_markers2.append((float(events[0]) + 200.0,str(events[1])))\n",
    " \n",
    "markers2 = np.array(train_markers2)\n",
    "y2 = markers2[:, 1]\n",
    "\n",
    "y2 = [int(value) for value in y2]\n",
    "\n",
    "\n",
    "# y2 = [0] * len(markers2)\n",
    "\n",
    "markers_subject2_class_1 = [(float(events[0]),str(events[1])) for events in markers2 if events[1]== '1']\n",
    "markers_subject2_class_2 = [(float(events[0]),str(events[1])) for events in markers2 if events[1]== '2']\n",
    "    \n",
    "cnt1 = convert_mushu_data(signal_array, markers_subject2_class_1,100,channels)\n",
    "cnt2 = convert_mushu_data(signal_array, markers_subject2_class_2,100,channels)\n",
    "\n",
    "\n",
    "    \n",
    "md = {'class 1': ['1'],'class 2': ['2']}\n",
    "    \n",
    "epoch_subject2_class1 = segment_dat(cnt1, md, [0, 1000])\n",
    "epoch_subject2_class2 = segment_dat(cnt2, md, [0, 1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for 3rd subject\n",
    "\n",
    "training_data = 'Training data/' + files[2] + '/data_set_IVa_' + files[2] + '_cnt.txt'\n",
    "markers       = 'Training data/' + files[2] + '/data_set_IVa_' + files[2] + '_mrk.txt'\n",
    "    \n",
    "signal_array = np.loadtxt(training_data)\n",
    "\n",
    "(b, a) = signal.butter(3, np.array([8, 30])/ 100.00, 'bandpass')\n",
    "signal_array = signal.lfilter(b, a,signal_array, 1)\n",
    "            \n",
    "#signal_array = np.transpose(signal_array)\n",
    "    \n",
    "# marker_array = [map(int,l.split()) for l in open(markers).readlines()]\n",
    "        \n",
    "time = np.arange(len(signal_array))\n",
    "markers3 = np.loadtxt(markers, dtype = int)\n",
    "train_markers3 = [(float(events[0]),str(events[1])) for events in markers3 if events[1]!= 0]\n",
    "\n",
    "\n",
    "for events in markers3:\n",
    "    if events[1] != 0:\n",
    "        train_markers3.append((float(events[0]) + 100.0,str(events[1])))\n",
    "        train_markers3.append((float(events[0]) + 200.0,str(events[1])))\n",
    "        \n",
    "markers3 = np.array(train_markers3)\n",
    "y3 = markers3[:, 1]\n",
    "\n",
    "y3 = [int(value) for value in y3]\n",
    "\n",
    "\n",
    "# y3 = [0] * len(markers3)\n",
    "\n",
    "markers_subject3_class_1 = [(float(events[0]),str(events[1])) for events in markers3 if events[1]== '1']\n",
    "markers_subject3_class_2 = [(float(events[0]),str(events[1])) for events in markers3 if events[1]== '2']\n",
    "\n",
    "    \n",
    "cnt1 = convert_mushu_data(signal_array, markers_subject3_class_1,100,channels)\n",
    "cnt2 = convert_mushu_data(signal_array, markers_subject3_class_2,100,channels)\n",
    "    \n",
    "md = {'class 1': ['1'],'class 2': ['2']}\n",
    "    \n",
    "epoch_subject3_class1 = segment_dat(cnt1, md, [0, 1000])\n",
    "epoch_subject3_class2 = segment_dat(cnt2, md, [0, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 4th subject\n",
    "\n",
    "training_data = 'Training data/' + files[3] + '/data_set_IVa_' + files[3] + '_cnt.txt'\n",
    "markers       = 'Training data/' + files[3] + '/data_set_IVa_' + files[3] + '_mrk.txt'\n",
    "    \n",
    "signal_array = np.loadtxt(training_data)\n",
    "\n",
    "(b, a) = signal.butter(3, np.array([8, 30])/ 100.00, 'bandpass')\n",
    "signal_array = signal.lfilter(b, a,signal_array, 1)\n",
    "            \n",
    "#signal_array = np.transpose(signal_array)\n",
    "    \n",
    "# marker_array = [map(int,l.split()) for l in open(markers).readlines()]\n",
    "        \n",
    "time = np.arange(len(signal_array))\n",
    "markers4 = np.loadtxt(markers, dtype = int)   \n",
    "train_markers4 = [(float(events[0]),str(events[1])) for events in markers4 if events[1]!= 0]\n",
    "\n",
    "\n",
    "for events in markers4:\n",
    "    if events[1] != 0:\n",
    "        train_markers4.append((float(events[0])+100.0,str(events[1])))\n",
    "        train_markers4.append((float(events[0])+200.0,str(events[1])))\n",
    "        \n",
    "markers4 = np.array(train_markers4)\n",
    "y4 = markers4[:, 1]\n",
    "\n",
    "y4 = [int(value) for value in y4]\n",
    "\n",
    "\n",
    "# y4 = [0] * len(markers4)\n",
    "\n",
    "\n",
    "markers_subject4_class_1 = [(float(events[0]),str(events[1])) for events in markers4 if events[1]== '1']\n",
    "markers_subject4_class_2 = [(float(events[0]),str(events[1])) for events in markers4 if events[1]== '2']\n",
    "    \n",
    "    \n",
    "cnt1 = convert_mushu_data(signal_array, markers_subject4_class_1, 100, channels)\n",
    "cnt2 = convert_mushu_data(signal_array, markers_subject4_class_2, 100, channels)\n",
    "    \n",
    "md = {'class 1': ['1'], 'class 2': ['2']}\n",
    "    \n",
    "epoch_subject4_class1 = segment_dat(cnt1, md, [0, 1000])\n",
    "epoch_subject4_class2 = segment_dat(cnt2, md, [0, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for 5th subject\n",
    "\n",
    "training_data = 'Training data/' + files[4] + '/data_set_IVa_' + files[4] + '_cnt.txt'\n",
    "markers       = 'Training data/' + files[4] + '/data_set_IVa_' + files[4] + '_mrk.txt'\n",
    "    \n",
    "signal_array = np.loadtxt(training_data)\n",
    "\n",
    "(b, a) = signal.butter(3, np.array([8, 30])/ 100.00, 'bandpass')\n",
    "signal_array = signal.lfilter(b, a,signal_array, 1)\n",
    "            \n",
    "#signal_array = np.transpose(signal_array)\n",
    "    \n",
    "# marker_array = [map(int,l.split()) for l in open(markers).readlines()]\n",
    "        \n",
    "time = np.arange(len(signal_array))\n",
    "markers5 = np.loadtxt(markers, dtype = int)      \n",
    "train_markers5 = [(float(events[0]),str(events[1])) for events in markers5 if events[1]!= 0]\n",
    "\n",
    "\n",
    "for events in markers5:\n",
    "    if events[1] != 0:\n",
    "        train_markers5.append((float(events[0]) + 100.0, str(events[1])))\n",
    "        train_markers5.append((float(events[0]) + 200.0, str(events[1])))\n",
    "        \n",
    "markers5 = np.array(train_markers5)\n",
    "y5 = markers5[:, 1]\n",
    "\n",
    "y5 = [int(value) for value in y5]\n",
    "\n",
    "# y5 = [0] * len(markers5)\n",
    "\n",
    "markers_subject5_class_1 = [(float(events[0]),str(events[1])) for events in markers5 if events[1]== '1']\n",
    "markers_subject5_class_2 = [(float(events[0]),str(events[1])) for events in markers5 if events[1]== '2']\n",
    "\n",
    "    \n",
    "    \n",
    "cnt1 = convert_mushu_data(signal_array, markers_subject5_class_1,100,channels)\n",
    "cnt2 = convert_mushu_data(signal_array, markers_subject5_class_2,100,channels)\n",
    "    \n",
    "md = {'class 1': ['1'],'class 2': ['2']}\n",
    "    \n",
    "epoch_subject5_class1 = segment_dat(cnt1, md, [0, 1000])\n",
    "epoch_subject5_class2 = segment_dat(cnt2, md, [0, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.28032167,  -8.74874224, -16.39737541, ..., -28.13124447,\n",
       "        -28.14914055, -14.52003294],\n",
       "       [ -1.9854922 , -10.36341916, -25.7352505 , ..., -25.8881739 ,\n",
       "        -24.93362325, -11.5020445 ],\n",
       "       [ -2.05459286,  -7.63703252, -13.92491319, ..., -29.08261514,\n",
       "        -26.63405158, -13.4150343 ],\n",
       "       ..., \n",
       "       [ -1.27605879,  -6.42920443, -16.26957697, ..., -23.5003638 ,\n",
       "        -20.99038285,  -9.25479407],\n",
       "       [ -1.41886682,  -4.2613752 ,  -6.1449352 , ..., -22.45830974,\n",
       "        -20.35207302,  -9.54002527],\n",
       "       [ -1.15167761,  -5.97264674, -15.5685926 , ..., -22.14528795,\n",
       "        -20.43684071,  -9.27009135]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appending all the different subject training epochs\n",
    "\n",
    "temp1_epo1 = append_epo(epoch_subject1_class1, epoch_subject2_class1)\n",
    "temp1_epo2 = append_epo(epoch_subject3_class1, epoch_subject4_class1)\n",
    "temp1_epo3 = append_epo(temp1_epo1, temp1_epo2)\n",
    "class1_epochs = append_epo(temp1_epo3, epoch_subject5_class1)\n",
    "\n",
    "temp2_epo1 = append_epo(epoch_subject1_class2, epoch_subject2_class2)\n",
    "temp2_epo2 = append_epo(epoch_subject3_class2, epoch_subject4_class2)\n",
    "temp2_epo3 = append_epo(temp2_epo1, temp2_epo2)\n",
    "class2_epochs = append_epo(temp2_epo3, epoch_subject5_class2)\n",
    "\n",
    "\n",
    "final_epoch = append_epo(class1_epochs, class2_epochs)\n",
    "s = final_epoch.data[1]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### FUNCTION TO FIND BAND POWERS ###########\n",
    "\n",
    "def bandpowers(segment):\n",
    "    features = []\n",
    "    \n",
    "    for i in range(len(segment)):\n",
    "        f,Psd = signal.welch(segment[i,:], 100)\n",
    "        power1 = 0\n",
    "        power2 = 0\n",
    "        f1 = []\n",
    "        \n",
    "        for j in range(0,len(f)):\n",
    "            if(f[j] >= 4 and f[j] <= 13):\n",
    "                power1 += Psd[j]\n",
    "            if(f[j] >= 14 and f[j] <= 30):\n",
    "                power2 += Psd[j]\n",
    "        features.append(power1)\n",
    "        features.append(power2)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Discrete Cosine Transform #########\n",
    "from scipy.fftpack import fft, dct\n",
    "\n",
    "def dct_features(segment):\n",
    "    features = []\n",
    "    \n",
    "    for i in range(len(segment)):\n",
    "        dct_coef = dct(segment[i,:], 2, norm='ortho')\n",
    "        power = sum( j*j for j in dct_coef)\n",
    "        features.append(power)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### FUNCTION TO FIN WAVELET FEATURES #######\n",
    "\n",
    "def wavelet_features(epoch):\n",
    "    cA_values = []\n",
    "    cD_values = []\n",
    "    cA_mean = []\n",
    "    cA_std = []\n",
    "    cA_Energy =[]\n",
    "    cD_mean = []\n",
    "    cD_std = []\n",
    "    cD_Energy = []\n",
    "    Entropy_D = []\n",
    "    Entropy_A = []\n",
    "    features = []\n",
    "    for i in range(len(epoch)):\n",
    "        cA,cD=pywt.dwt(epoch[i,:],'coif1')\n",
    "        cA_values.append(cA)\n",
    "        cD_values.append(cD)\t\t#calculating the coefficients of wavelet transform.\n",
    "    for x in range(len(epoch)):   \n",
    "        cA_Energy.append(abs(np.sum(np.square(cA_values[x]))))\n",
    "        features.append(abs(np.sum(np.square(cA_values[x]))))\n",
    "        \n",
    "    for x in range(len(epoch)):      \n",
    "        cD_Energy.append(abs(np.sum(np.square(cD_values[x]))))\n",
    "        features.append(abs(np.sum(np.square(cD_values[x]))))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\scipy\\signal\\spectral.py:1637: UserWarning: nperseg = 256 is greater than input length  = 100, using nperseg = 100\n",
      "  .format(nperseg, input_length))\n"
     ]
    }
   ],
   "source": [
    "## FULL FEATURES DICTIONARY\n",
    "\n",
    "# Dictionary with band power features\n",
    "#form the dictionary using bandpower features where each column is a data point.\n",
    "\n",
    "#For class 1\n",
    "dictionary1 = []\n",
    "dictionary2 = []\n",
    "for i in range(len(final_epoch.axes[0])):\n",
    "    segment = final_epoch.data[i]\n",
    "    segment = np.array(segment)\n",
    "    segment = np.transpose(segment)\n",
    "    \n",
    "    #features = dct_features(segment)\n",
    "    features1 = bandpowers(segment)  ## Comment the dictionaries not used\n",
    "    #features = logvariance(segment)\n",
    "    \n",
    "    features2 = wavelet_features(segment)\n",
    "    \n",
    "    dictionary1.append(features1)\n",
    "    dictionary2.append(features2)\n",
    "    \n",
    "    \n",
    "dictionary1 = np.array(dictionary1)\n",
    "dictionary2 = np.array(dictionary2)\n",
    "\n",
    "\n",
    "dictionary_bandpower  = dictionary1\n",
    "dictionary_wavelet = dictionary2\n",
    "\n",
    "#dictionary_dct = dictionary1\n",
    "\n",
    "\n",
    "targets = final_epoch.axes[0]\n",
    "\n",
    "# dictionary1_rh = dictionary1[:,:118]\n",
    "# # dictionary1_rl = dictionary1.iloc[:,-118:]\n",
    "\n",
    "# dictionary1_rh\n",
    "#dictionary1.shape\n",
    "\n",
    "# dictionary1[0], dictionary1[118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target = y1 + y2 + y3 + y4 + y5\n",
    "len(y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65476190476190477"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state = 40)\n",
    "\n",
    "modelSVM = svm.SVC()\n",
    "modelSVM.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred = modelSVM.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize= True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_transpose = np.transpose(s)\n",
    "feature_s = bandpowers(s_transpose)\n",
    "len(feature_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLYING ML ALGORITHMS FOR BAND POWER FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65476190476190477"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state = 40)\n",
    "\n",
    "modelSVM = svm.SVC()\n",
    "modelSVM.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred = modelSVM.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize= True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAUSSIAN NAIVE BAYES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state=30)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI LAYER PERCEPTRON CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47619047619047616"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size = 0.1, random_state=40)\n",
    "\n",
    "modelMLP = MLPClassifier(solver='lbfgs', alpha = 1e-5, hidden_layer_sizes = (5, 2), random_state=1)\n",
    " \n",
    "modelMLP.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = modelMLP.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADA BOOST CLASSIFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50892857142857151"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "modelAda = AdaBoostClassifier(n_estimators = 200)\n",
    "scores = cross_val_score(modelAda, dictionary_bandpower, y_target)\n",
    "scores.mean()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR DISCRIMINANT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59523809523809523"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.lda import LDA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state=40)\n",
    "\n",
    "modelLDA = LDA()\n",
    "\n",
    "modelLDA.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = modelLDA.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52976190476190477"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state=40)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535714285714286"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state=40)\n",
    "\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators = 200)\n",
    "\n",
    "bdt.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = bdt.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize=True, sample_weight=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51726190476190481"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "modelRF = RandomForestClassifier(n_estimators = 5, max_depth=None, min_samples_split=3, random_state=10)\n",
    "scores = cross_val_score(modelRF, dictionary_bandpower, y_target)\n",
    "scores.mean()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nu SUPPORT VECTOR CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63690476190476186"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state=40)\n",
    "modelSVM = svm.NuSVC()\n",
    "modelSVM.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred = modelSVM.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dictionary_bandpower, y_target, test_size=0.1, random_state=40)\n",
    "\n",
    "modelDecisionTree = tree.DecisionTreeClassifier()\n",
    "modelDecisionTree = modelDecisionTree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_test_pred = modelDecisionTree.predict(X_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTINUED IN THE NEXT NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING ONLY 30 CHANNELS PRESENT ON THE MOTOR CORTEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Fp1',\n",
       "  'AFp1',\n",
       "  'Fpz',\n",
       "  'AFp2',\n",
       "  'Fp2',\n",
       "  'AF7',\n",
       "  'AF3',\n",
       "  'AF4',\n",
       "  'AF8',\n",
       "  'FAF5',\n",
       "  'FAF1',\n",
       "  'FAF2',\n",
       "  'FAF6',\n",
       "  'F7',\n",
       "  'F5',\n",
       "  'F3',\n",
       "  'F1',\n",
       "  'Fz',\n",
       "  'F2',\n",
       "  'F4',\n",
       "  'F6',\n",
       "  'F8',\n",
       "  'FFC7',\n",
       "  'FFC5',\n",
       "  'FFC3',\n",
       "  'FFC1',\n",
       "  'FFC2',\n",
       "  'FFC4',\n",
       "  'FFC6',\n",
       "  'FFC8',\n",
       "  'FT9',\n",
       "  'FT7',\n",
       "  'FC5',\n",
       "  'FC3',\n",
       "  'FC1',\n",
       "  'FCz',\n",
       "  'FC2',\n",
       "  'FC4',\n",
       "  'FC6',\n",
       "  'FT8',\n",
       "  'FT10',\n",
       "  'CFC7',\n",
       "  'CFC5',\n",
       "  'CFC3',\n",
       "  'CFC1',\n",
       "  'CFC2',\n",
       "  'CFC4',\n",
       "  'CFC6',\n",
       "  'CFC8',\n",
       "  'T7',\n",
       "  'C5',\n",
       "  'C3',\n",
       "  'C1',\n",
       "  'Cz',\n",
       "  'C2',\n",
       "  'C4',\n",
       "  'C6',\n",
       "  'T8',\n",
       "  'CCP7',\n",
       "  'CCP5',\n",
       "  'CCP3',\n",
       "  'CCP1',\n",
       "  'CCP2',\n",
       "  'CCP4',\n",
       "  'CCP6',\n",
       "  'CCP8',\n",
       "  'TP9',\n",
       "  'TP7',\n",
       "  'CP5',\n",
       "  'CP3',\n",
       "  'CP1',\n",
       "  'CPz',\n",
       "  'CP2',\n",
       "  'CP4',\n",
       "  'CP6',\n",
       "  'TP8',\n",
       "  'TP10',\n",
       "  'PCP7',\n",
       "  'PCP5',\n",
       "  'PCP3',\n",
       "  'PCP1',\n",
       "  'PCP2',\n",
       "  'PCP4',\n",
       "  'PCP6',\n",
       "  'PCP8',\n",
       "  'P9',\n",
       "  'P7',\n",
       "  'P5',\n",
       "  'P3',\n",
       "  'P1',\n",
       "  'Pz',\n",
       "  'P2',\n",
       "  'P4',\n",
       "  'P6',\n",
       "  'P8',\n",
       "  'P10',\n",
       "  'PPO7',\n",
       "  'PPO5',\n",
       "  'PPO1',\n",
       "  'PPO2',\n",
       "  'PPO6',\n",
       "  'PPO8',\n",
       "  'PO7',\n",
       "  'PO3',\n",
       "  'PO1',\n",
       "  'POz',\n",
       "  'PO2',\n",
       "  'PO4',\n",
       "  'PO8',\n",
       "  'OPO1',\n",
       "  'OPO2',\n",
       "  'O1',\n",
       "  'Oz',\n",
       "  'O2',\n",
       "  'OI1',\n",
       "  'OI2',\n",
       "  'I1',\n",
       "  'I2'],\n",
       " 118)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels, len(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels_motor = [ 'FC2', 'FC4', 'FC6', 'CFC2', 'CFC4', 'CFC6', 'C2', 'C4', 'C6', 'CCP2', 'CCP4', 'CCP6', 'CP2', \n",
    "                  'CP4', 'CP6', 'FC5', 'FC3', 'FC1', 'CFC5', 'CFC3', 'CFC1', 'C5', 'C3', 'C1', 'CCP5', 'CCP3', 'CCP1', \n",
    "                  'CP5', 'CP3', 'CP1']\n",
    "len(channels_motor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283562L, 118L)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 33,\n",
       " 34,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 72,\n",
       " 73,\n",
       " 74]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = []\n",
    "\n",
    "for i, j in enumerate(channels):\n",
    "    for k in channels_motor:\n",
    "        if j == k:\n",
    "            indices.append(i)\n",
    "            \n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File training_data does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-3eb878480014>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BCI-HCI-LAB-1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3427)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:6861)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File training_data does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('training_data', header = None, names = channels)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
